# ğŸ¦ Insurance Claim Amount Prediction (End-to-End ML System)

An **end-to-end, production-ready Machine Learning system** for predicting insurance claim amounts using **Python, FastAPI, Streamlit, Docker, and CI/CD**.

This project demonstrates **industry-level ML engineering practices**, including clean architecture, model versioning, API-based inference, containerization, and automated CI pipelines.

---

## ğŸ“Œ Business Problem

Insurance companies must estimate the **expected cost of a claim** before it is fully processed.
Accurate predictions help with:

* Premium pricing
* Risk management
* Reserve allocation
* Profitability analysis

### ğŸ¯ Objective

> Predict the **expected insurance claim amount** given customer, policy, and accident details.

---

## ğŸ“Š Model Inputs & Output

### ğŸ”¹ Input Features

* Age
* Annual income
* Vehicle age
* Past claims count
* Accident severity (1â€“5)
* Policy tenure (years)

### ğŸ”¹ Output

* **Estimated claim amount** (continuous value)

---

## ğŸ§  Machine Learning Approach

* **Model Type**: Regression (GLM-style / Linear Regression baseline)
* **Feature Scaling**: StandardScaler
* **Artifacts**:

  * Trained model (`model.pkl`)
  * Scaler (`scaler.pkl`)
  * Metadata (`metadata.json`)

Model artifacts are versioned and loaded dynamically at application startup.

---

## ğŸ—ï¸ System Architecture

```
User (Browser)
   â†“
Streamlit Frontend (UI)
   â†“
FastAPI Backend (Inference API)
   â†“
ML Model (Artifacts)
```

---

## ğŸ“‚ Project Structure

```
insurance-claim-prediction/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/            # API routes
â”‚   â”‚   â”œâ”€â”€ services/       # Prediction & model loading logic
â”‚   â”‚   â”œâ”€â”€ schemas/        # Pydantic request/response schemas
â”‚   â”‚   â”œâ”€â”€ core/           # Config & logging
â”‚   â”‚   â””â”€â”€ main.py         # FastAPI app entrypoint
â”‚   â”‚
â”‚   â”œâ”€â”€ artifacts/          # Model, scaler, metadata
â”‚   â”œâ”€â”€ tests/              # Pytest test suite
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ streamlit_app.py    # Streamlit UI
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ docker-compose.yml      # Multi-service orchestration
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci.yml              # CI pipeline (pytest + docker compose)
â”‚
â””â”€â”€ README.md
```

---

## ğŸš€ How to Run Locally (Docker Recommended)

### 1ï¸âƒ£ Prerequisites

* Docker Desktop
* Docker Compose (v2+)

Verify:

```bash
docker --version
docker compose version
```

---

### 2ï¸âƒ£ Start the System

From project root:

```bash
docker compose up --build
```

---

### 3ï¸âƒ£ Access Applications

* **FastAPI Swagger UI**
  ğŸ‘‰ [http://localhost:8000/docs](http://localhost:8000/docs)

* **Health Check**
  ğŸ‘‰ [http://localhost:8000/health](http://localhost:8000/health)

* **Streamlit Frontend**
  ğŸ‘‰ [http://localhost:8501](http://localhost:8501)

---

## ğŸ” API Example

### POST `/predict`

**Request**

```json
{
  "age": 40,
  "annual_income": 80000,
  "vehicle_age": 6,
  "past_claims": 1,
  "accident_severity": 4,
  "policy_tenure": 5
}
```

**Response**

```json
{
  "estimated_claim_amount": 12345.67
}
```

---

## ğŸ§ª Testing Strategy

### âœ” Unit & Integration Tests

* Schema validation
* Predictor service tests
* API endpoint tests

Run locally:

```bash
cd backend
pytest -v
```

---

## ğŸ” CI/CD Pipeline

This project includes a **production-grade CI pipeline** using **GitHub Actions**.

### CI Steps:

1. Run backend tests (`pytest`)
2. Build Docker images
3. Start full system using Docker Compose
4. Wait for backend health
5. Run inference smoke test
6. Tear down containers

### CI guarantees:

* Code quality
* Model compatibility
* Container correctness
* End-to-end system stability

---

## ğŸ³ Containerization

* Backend and frontend are **separate Docker images**
* Services communicate via **Docker internal networking**
* Health-based startup ordering
* Lightweight `python:3.12-slim` images

---

## ğŸ” Engineering Best Practices Used

* Clean architecture & separation of concerns
* No model loading at import time
* Centralized configuration
* Structured logging
* Strong input validation
* Reproducible builds
* Health & readiness checks
* CI automation

---

## ğŸ“ˆ Possible Extensions

* Model retraining pipeline
* Data drift monitoring
* Authentication & rate limiting
* Cloud deployment (AWS/GCP)
* Feature store integration
* Advanced insurance GLMs

---

## ğŸ¯ Why This Project Matters

This is **not a toy ML notebook**.

It demonstrates:

* Real ML inference service design
* Backend engineering best practices
* Production-ready CI/CD
* Full-stack ML system thinking

---

## ğŸ‘¨â€ğŸ’» Author

Built as part of **â€œCode Like a Proâ€** learning journey
by an AI/ML Engineer focusing on **production-grade ML systems**.

---

## â­ Final Note

If you are reviewing this project:

* Start with `docker compose up`
* Explore `/docs`
* Check `.github/workflows/ci.yml`
* Review tests and architecture

This repository reflects **how ML systems are built in real companies**.

---
